{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:22:31.576129Z",
     "start_time": "2025-02-27T21:22:26.832547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install nltk\n",
    "!pip install numpy\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Base do WordNet\n"
   ],
   "id": "be92ed0cb02a627c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/alex/miniconda3/envs/pythonProject1/lib/python3.11/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: click in /home/alex/miniconda3/envs/pythonProject1/lib/python3.11/site-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /home/alex/miniconda3/envs/pythonProject1/lib/python3.11/site-packages (from nltk) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/alex/miniconda3/envs/pythonProject1/lib/python3.11/site-packages (from nltk) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /home/alex/miniconda3/envs/pythonProject1/lib/python3.11/site-packages (from nltk) (4.67.1)\r\n",
      "Collecting numpy\r\n",
      "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\r\n",
      "Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.4/16.4 MB\u001B[0m \u001B[31m36.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: numpy\r\n",
      "Successfully installed numpy-2.2.3\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alex/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Tokenização\n",
    "* Objetivo: Separar um texto em palavras e sentenças.\n",
    "* Tarefa:\n",
    "    - Execute o código e analise os resultados.\n",
    "    - Modifique o texto de entrada e veja se há mudanças no comportamento da tokenização.\n"
   ],
   "id": "bad44a1d69e8e80b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-27T21:21:08.721067Z",
     "start_time": "2025-02-27T21:21:08.640567Z"
    }
   },
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Baixar os pacotes necessários\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"Olá! Bem-vindo ao curso de PLN. Hoje vamos aprender sobre tokenização.\"\n",
    "\n",
    "# Tokenização por palavras\n",
    "palavras = word_tokenize(texto)\n",
    "print(\"Tokens por palavras:\", palavras)\n",
    "\n",
    "# Tokenização por sentenças\n",
    "sentencas = sent_tokenize(texto)\n",
    "print(\"Tokens por sentenças:\", sentencas)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens por palavras: ['Olá', '!', 'Bem-vindo', 'ao', 'curso', 'de', 'PLN', '.', 'Hoje', 'vamos', 'aprender', 'sobre', 'tokenização', '.']\n",
      "Tokens por sentenças: ['Olá!', 'Bem-vindo ao curso de PLN.', 'Hoje vamos aprender sobre tokenização.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Remoção de Stopwords\n",
    "* Objetivo: Filtrar palavras irrelevantes para análise.\n",
    "* Tarefa:\n",
    "    * Teste a remoção de stopwords com diferentes frases.\n",
    "    * Adicione palavras personalizadas à lista de stopwords."
   ],
   "id": "5d4522563734bd38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:21:18.212046Z",
     "start_time": "2025-02-27T21:21:18.092153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar a lista de stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Definir stopwords em português\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Remover stopwords do texto tokenizado\n",
    "palavras_filtradas = [palavra for palavra in palavras if palavra.lower() not in stop_words]\n",
    "\n",
    "print(\"Texto sem stopwords:\", palavras_filtradas)\n"
   ],
   "id": "d650a8ed4af708e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sem stopwords: ['Olá', '!', 'Bem-vindo', 'curso', 'PLN', '.', 'Hoje', 'vamos', 'aprender', 'sobre', 'tokenização', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Stemming\n",
    "* Objetivo: Reduzir palavras às suas raízes.\n",
    "* Tarefa:\n",
    "    * Compare os resultados do PorterStemmer e do RSLPStemmer.\n",
    "    * Acrescente mais palavras e analise os resultados."
   ],
   "id": "97f46f6e2695b084"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:21:25.594160Z",
     "start_time": "2025-02-27T21:21:25.579978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import PorterStemmer, RSLPStemmer\n",
    "\n",
    "# Criar os stemmers\n",
    "porter = PorterStemmer()\n",
    "rslp = RSLPStemmer()\n",
    "\n",
    "# Aplicar stemming em palavras\n",
    "palavras_exemplo = [\"correndo\", \"correu\", \"correrão\", \"jogando\", \"jogador\"]\n",
    "\n",
    "print(\"Stemming - Porter:\", [porter.stem(p) for p in palavras_exemplo])\n",
    "print(\"Stemming - RSLP:\", [rslp.stem(p) for p in palavras_exemplo])\n"
   ],
   "id": "7291d547e48958d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming - Porter: ['correndo', 'correu', 'correrão', 'jogando', 'jogador']\n",
      "Stemming - RSLP: ['corr', 'corr', 'corr', 'jog', 'jog']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Lematização\n",
    "* Objetivo: Reduzir palavras à sua forma base considerando o contexto.\n",
    "* Tarefa:\n",
    "    * Teste a lematização com verbos e substantivos em inglês.\n",
    "    * Compare os resultados da lematização e do stemming."
   ],
   "id": "205f102ff04a9174"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:21:35.370576Z",
     "start_time": "2025-02-27T21:21:33.045397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Baixar os pacotes necessários\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Criar o lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Exemplos de palavras para lematização\n",
    "palavras_exemplo = [\"correndo\", \"correu\", \"correrão\", \"jogando\", \"jogador\"]\n",
    "\n",
    "# Aplicando lematização\n",
    "print(\"Lematização:\", [lemmatizer.lemmatize(p) for p in palavras_exemplo])\n"
   ],
   "id": "69efdfc6e0f481fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/alex/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematização: ['correndo', 'correu', 'correrão', 'jogando', 'jogador']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 5. POS-Tagging (Etiquetagem Gramatical)\n",
    "* Objetivo: Identificar a categoria gramatical de cada palavra.\n",
    "* Tarefa:\n",
    "    * Identifique a categoria gramatical das palavras de uma frase diferente.\n",
    "    * Traduzir para o português e observar a necessidade de modelos em português.\n"
   ],
   "id": "ccd98977c752f729"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:21:41.288048Z",
     "start_time": "2025-02-27T21:21:41.157626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# Baixar pacotes necessários\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Exemplo de frase em inglês\n",
    "frase = word_tokenize(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "# Aplicar POS-Tagging\n",
    "print(\"POS-Tagging:\", pos_tag(frase))\n"
   ],
   "id": "1d5aac4b7768e82d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS-Tagging: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alex/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 6. Reconhecimento de Entidades Nomeadas (NER)\n",
    "* Objetivo: Extrair nomes de pessoas, locais e organizações de um texto.\n",
    "* Tarefa:\n",
    "    * Teste o NER com nomes de pessoas e empresas brasileiras.\n",
    "    * Observe os tipos de entidades reconhecidas."
   ],
   "id": "f3d6f3d6ceb3327e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T21:22:37.374291Z",
     "start_time": "2025-02-27T21:22:37.000271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "# Baixar pacotes necessários\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Exemplo de frase para NER\n",
    "frase = pos_tag(word_tokenize(\"Barack Obama was the president of the United States.\"))\n",
    "\n",
    "# Aplicar NER\n",
    "print(\"NER:\", ne_chunk(frase))\n"
   ],
   "id": "2fbd6dfa629694e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/alex/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/alex/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER: (S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  president/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
